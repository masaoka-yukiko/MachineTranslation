{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "231a42e0"
      },
      "source": [
        "Цель исследования - попробовать написать модель машинного перевода, познакомиться с инструментами машинного перевода"
      ],
      "id": "231a42e0"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9d1e46c5"
      },
      "outputs": [],
      "source": [
        "# импорты\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import WordNetLemmatizer\n",
        "from nltk.translate import AlignedSent, Alignment\n",
        "from nltk.translate.phrase_based import phrase_extraction\n",
        "import spacy"
      ],
      "id": "9d1e46c5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5ZzoefshJqx",
        "outputId": "97299002-8590-459f-c6ac-b7b5ef405e5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "id": "Z5ZzoefshJqx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb_UqkpYCZkm"
      },
      "source": [
        "# Создание своей модели"
      ],
      "id": "Pb_UqkpYCZkm"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "50da140d"
      },
      "outputs": [],
      "source": [
        "# прочитаем первый словарь\n",
        "with open('minidict.csv', newline='') as csvfile:\n",
        "  reader = csv.DictReader(csvfile)\n",
        "  words = dict()\n",
        "  for row in reader:\n",
        "    words[row ['eng']] = row['ru']"
      ],
      "id": "50da140d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0gzIlxRdqs1"
      },
      "source": [
        "Сначала попробуем создать модель, которая переводит слова"
      ],
      "id": "o0gzIlxRdqs1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwMfTbOaQqgr"
      },
      "outputs": [],
      "source": [
        "# перевод слов\n",
        "class wordmodel:\n",
        "    def __init__(self): \n",
        "        print(\"I am word translator\")\n",
        "        \n",
        "    def fit(self, words):\n",
        "        # запомним соответствия по словам\n",
        "        self.w = words\n",
        "    \n",
        "    def predict(self, word):\n",
        "      # найдем слово в списке\n",
        "        if word in self.w.keys():\n",
        "          return self.w[word]\n",
        "        else:\n",
        "          # возможно, есть однокоренные слова\n",
        "          # поищем строку\n",
        "          for elem in self.w:\n",
        "            if word in elem:\n",
        "              return self.w[elem] + '-some_gloss' \n",
        "          else:\n",
        "            return word\n",
        "            print('Непереводимое слово')"
      ],
      "id": "DwMfTbOaQqgr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "VKY_lYNUcmAs",
        "outputId": "839f1ba6-4071-4ccf-f4de-6f94d5ea8bb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am word translator\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'кошка-some_gloss'"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w1 = wordmodel()\n",
        "w1.fit(words)\n",
        "w1.predict('cat')\n",
        "w1.predict('ca')"
      ],
      "id": "VKY_lYNUcmAs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aid-zjV3dY3k"
      },
      "source": [
        "Следующий шаг - модель, которая может работать с несколькими словами"
      ],
      "id": "Aid-zjV3dY3k"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01c58cee"
      },
      "outputs": [],
      "source": [
        "# модель с токенизацией и лемматизацией\n",
        "class sentmodel:\n",
        "    def __init__(self): \n",
        "        print(\"I am machine translator\")\n",
        "        \n",
        "    def fit(self, words):\n",
        "        # запомним соответствия по словам\n",
        "        self.w = words\n",
        "        \n",
        "    def predict(self, string):\n",
        "        # токенизируем строку\n",
        "        ws = [w.lower() for w in word_tokenize(string) if w.isalpha()]\n",
        "        wnl = WordNetLemmatizer()\n",
        "        trans = [] # массив для перевода\n",
        "        for tok in ws:\n",
        "          # лемматизируем каждый токен\n",
        "          lemma = wnl.lemmatize(tok)\n",
        "          # найдем слово в словаре\n",
        "          if lemma in self.w.keys():\n",
        "            trans.append(self.w[lemma])\n",
        "          else:\n",
        "            # возможно, есть однокоренные слова\n",
        "            # поищем строку\n",
        "            for elem in self.w:\n",
        "              if lemma in elem:\n",
        "                trans.append(self.w[elem] + '-some_gloss')\n",
        "            else:\n",
        "              trans.append('???')\n",
        "        answ = ''\n",
        "        for elem in trans:\n",
        "            answ += elem + ' '\n",
        "        return(answ)"
      ],
      "id": "01c58cee"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "UuhLEftJgp6i",
        "outputId": "9d7bfcf1-fb42-44dd-eaa8-b2ef863c1b8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am machine translator\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'кошка ??? собака '"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w2 = sentmodel()\n",
        "w2.fit(words)\n",
        "w2.predict('cat eats dog')"
      ],
      "id": "UuhLEftJgp6i"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ4BwIR1dXcM"
      },
      "source": [
        "Проблемы данного подхода:\n",
        "1. Разбор слова - лемматайзер не находит лемму без указания части речи\n",
        "2. Обработка перевода - необходимо как-то учесть морфосинтаксис целевого языка\n",
        "3. Ограниченность словарем"
      ],
      "id": "eZ4BwIR1dXcM"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "7mY79wn0jVu6"
      },
      "outputs": [],
      "source": [
        "class syntmodel:\n",
        "  def __init__(self): \n",
        "        print(\"I am a smarter machine translator\")\n",
        "        \n",
        "  def fit(self, words):\n",
        "    # запомним соответствия по словам\n",
        "    self.w = words\n",
        "\n",
        "  def predict(self, string):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    wnl = WordNetLemmatizer()\n",
        "    d = nlp(string)\n",
        "    for token in d:\n",
        "      trans = []\n",
        "      dependencies = []\n",
        "      # найдем слово в словаре\n",
        "      if token.lemma_ in self.w.keys():\n",
        "        trans.append(self.w[token.lemma_])\n",
        "        dependencies.append(token.dep_)\n",
        "      else:\n",
        "        # возможно, есть однокоренные слова\n",
        "        # поищем строку\n",
        "        for elem in self.w:\n",
        "          if token.lemma_ in elem:\n",
        "            trans.append(self.w[elem] + '-some_gloss')\n",
        "            dependencies.append(token.dep_)\n",
        "          else:\n",
        "            trans.append('???')\n",
        "            dependencies.append('???')\n",
        "        answ = ''\n",
        "        for elem in trans:\n",
        "            answ += elem + ' '\n",
        "        return(answ)"
      ],
      "id": "7mY79wn0jVu6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQcs2Q6x_BHL"
      },
      "source": [
        "Проблемы данного подхода:\n",
        "1. Обработка перевода - необходимо как-то учесть морфосинтаксис целевого языка; для синтаксического парсинга необходимо точно быть уверенным в ветвлении языка\n",
        "2. Ограниченность словарем"
      ],
      "id": "cQcs2Q6x_BHL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaVnQtjC-qHG"
      },
      "source": [
        "Как решаются эти проблемы?\n",
        "1. Alignment\n",
        "2. Обучение на корпусных данных вместо использования словарей"
      ],
      "id": "XaVnQtjC-qHG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ6keVOlCTHk"
      },
      "source": [
        "# Обучение существующей модели\n",
        "на материале Библии"
      ],
      "id": "hJ6keVOlCTHk"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SwxdhXCp2ox0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac798fb-20b9-4669-c94b-a9a68f072e77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fast_align'...\n",
            "remote: Enumerating objects: 213, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 213 (delta 2), reused 4 (delta 2), pack-reused 204\u001b[K\n",
            "Receiving objects: 100% (213/213), 70.68 KiB | 5.89 MiB/s, done.\n",
            "Resolving deltas: 100% (110/110), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/clab/fast_align.git"
      ],
      "id": "SwxdhXCp2ox0"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSqU7lOW2o2J",
        "outputId": "1f8ce066-2b65-47ee-8960-74a58aaa07fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fast_align/build\n",
            "-- The C compiler identification is GNU 7.5.0\n",
            "-- The CXX compiler identification is GNU 7.5.0\n",
            "-- Check for working C compiler: /usr/bin/cc\n",
            "-- Check for working C compiler: /usr/bin/cc -- works\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++\n",
            "-- Check for working CXX compiler: /usr/bin/c++ -- works\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Could NOT find SparseHash (missing: SPARSEHASH_INCLUDE_DIR) \n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/fast_align/build\n",
            "\u001b[35m\u001b[1mScanning dependencies of target atools\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/atools.dir/src/alignment_io.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/atools.dir/src/atools.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX executable atools\u001b[0m\n",
            "[ 50%] Built target atools\n",
            "\u001b[35m\u001b[1mScanning dependencies of target fast_align\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/fast_align.dir/src/fast_align.cc.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/fast_align.dir/src/ttables.cc.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable fast_align\u001b[0m\n",
            "[100%] Built target fast_align\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "! mkdir /content/fast_align/build \n",
        "# создаем папку для сборки\n",
        "%cd fast_align/build \n",
        "# переходим в папку\n",
        "# ! pwd\n",
        "! cmake .. # собираем\n",
        "! make # собираем\n",
        "%cd /content/ \n",
        "# возвращаемся в домашнюю папку\n",
        "# ! pwd"
      ],
      "id": "gSqU7lOW2o2J"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2i0bVyyw8Fym",
        "outputId": "8b1e57dc-b7c3-4e41-8d28-0b4b798f08e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-30 20:26:59--  https://opus.nlpl.eu/download.php?f=bible-uedin/v1/moses/en-ru.txt.zip\n",
            "Resolving opus.nlpl.eu (opus.nlpl.eu)... 193.166.25.9\n",
            "Connecting to opus.nlpl.eu (opus.nlpl.eu)|193.166.25.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://object.pouta.csc.fi/OPUS-bible-uedin/v1/moses/en-ru.txt.zip [following]\n",
            "--2022-03-30 20:27:00--  https://object.pouta.csc.fi/OPUS-bible-uedin/v1/moses/en-ru.txt.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5809916 (5.5M) [application/zip]\n",
            "Saving to: ‘download.php?f=bible-uedin%2Fv1%2Fmoses%2Fen-ru.txt.zip.1’\n",
            "\n",
            "download.php?f=bibl 100%[===================>]   5.54M  5.19MB/s    in 1.1s    \n",
            "\n",
            "2022-03-30 20:27:02 (5.19 MB/s) - ‘download.php?f=bible-uedin%2Fv1%2Fmoses%2Fen-ru.txt.zip.1’ saved [5809916/5809916]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://opus.nlpl.eu/download.php?f=bible-uedin/v1/moses/en-ru.txt.zip"
      ],
      "id": "2i0bVyyw8Fym"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHbP_lbm8F70",
        "outputId": "c8848d27-1859-48ea-9930-539d53b836c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  download.php?f=bible-uedin%2Fv1%2Fmoses%2Fen-ru.txt.zip\n",
            "  inflating: README                  \n",
            "  inflating: LICENSE                 \n",
            "  inflating: bible-uedin.en-ru.en    \n",
            "  inflating: bible-uedin.en-ru.ru    \n",
            "  inflating: bible-uedin.en-ru.xml   \n"
          ]
        }
      ],
      "source": [
        "! unzip download.php?f=bible-uedin%2Fv1%2Fmoses%2Fen-ru.txt.zip"
      ],
      "id": "tHbP_lbm8F70"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "v0uTMDq78F-7"
      },
      "outputs": [],
      "source": [
        "! paste bible-uedin.en-ru.en bible-uedin.en-ru.ru  -d \"\\t\" | sed 's/\\t/ ||| /' > bible.en-ru"
      ],
      "id": "v0uTMDq78F-7"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIl-lKAi8GCP",
        "outputId": "16ff9fa1-f72e-4414-ee9b-acc0aabadb68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the beginning God created the heavens and the earth. ||| В начале сотворил Бог небо и землю.\n",
            "Now the earth was formless and empty. Darkness was on the surface of the deep. God's Spirit was hovering over the surface of the waters. ||| Земля же была безвидна и пуста, и тьма над бездною, и Дух Божий носился над водою.\n",
            "God said, \"Let there be light,\" and there was light. ||| И сказал Бог: да будет свет. И сталсвет.\n",
            "God saw the light, and saw that it was good. God divided the light from the darkness. ||| И увидел Бог свет, что он хорош, и отделил Бог свет от тьмы.\n",
            "God called the light \"day,\" and the darkness he called \"night.\" There was evening and there was morning, one day. ||| И назвал Бог свет днем, а тьму ночью. И был вечер, и было утро: день один.\n",
            "God said, \"Let there be an expanse in the middle of the waters, and let it divide the waters from the waters.\" ||| И сказал Бог: да будет твердь посреди воды, и да отделяет она воду от воды.\n",
            "God made the expanse, and divided the waters which were under the expanse from the waters which were above the expanse; and it was so. ||| И создал Бог твердь, и отделил воду, которая подтвердью, от воды, которая над твердью. И стало так.\n",
            "God called the expanse \"sky.\" There was evening and there was morning, a second day. ||| И назвал Бог твердь небом. И был вечер, и было утро: день второй.\n",
            "God said, \"Let the waters under the sky be gathered together to one place, and let the dry land appear\"; and it was so. ||| И сказал Бог: да соберется вода, которая под небом, в одно место, и да явится суша. И стало так.\n",
            "God called the dry land \"earth,\" and the gathering together of the waters he called \"seas.\" God saw that it was good. ||| И назвал Бог сушу землею, а собрание вод назвал морями. И увидел Бог, что это хорошо.\n"
          ]
        }
      ],
      "source": [
        "! head bible.en-ru"
      ],
      "id": "bIl-lKAi8GCP"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "k66OK91U8GFX"
      },
      "outputs": [],
      "source": [
        "! mv ./fast_align ./fast_align_git"
      ],
      "id": "k66OK91U8GFX"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xA9_5w-C8GJV"
      },
      "outputs": [],
      "source": [
        "! cp ./fast_align_git/build/fast_align ./fast_align\n",
        "! cp ./fast_align_git/build/atools ./atools"
      ],
      "id": "xA9_5w-C8GJV"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeJ7kwVO8alp",
        "outputId": "8507cc51-1a42-433f-8b4a-4231dfd66fdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARG=i\n",
            "ARG=d\n",
            "ARG=o\n",
            "ARG=v\n",
            "INITIAL PASS \n",
            ".................................................. [50000]\n",
            "............\n",
            "expected target length = source length * 0.742372\n",
            "ITERATION 1\n",
            ".................................................. [50000]\n",
            "............\n",
            "  log_e likelihood: -2.34116e+07\n",
            "  log_2 likelihood: -3.37758e+07\n",
            "     cross entropy: 29.8974\n",
            "        perplexity: 1e+09\n",
            "      posterior p0: 0.08\n",
            " posterior al-feat: -0.16928\n",
            "       size counts: 1424\n",
            "ITERATION 2\n",
            ".................................................. [50000]\n",
            "............\n",
            "  log_e likelihood: -8.04576e+06\n",
            "  log_2 likelihood: -1.16076e+07\n",
            "     cross entropy: 10.2747\n",
            "        perplexity: 1238.78\n",
            "      posterior p0: 0.044045\n",
            " posterior al-feat: -0.115135\n",
            "       size counts: 1424\n",
            "  1  model al-feat: -0.237202 (tension=4)\n",
            "  2  model al-feat: -0.171761 (tension=6.44134)\n",
            "  3  model al-feat: -0.151355 (tension=7.57387)\n",
            "  4  model al-feat: -0.140536 (tension=8.29828)\n",
            "  5  model al-feat: -0.13379 (tension=8.8063)\n",
            "  6  model al-feat: -0.129221 (tension=9.17941)\n",
            "  7  model al-feat: -0.125967 (tension=9.46114)\n",
            "  8  model al-feat: -0.123571 (tension=9.67779)\n",
            "     final tension: 9.84652\n",
            "ITERATION 3\n",
            ".................................................. [50000]\n",
            "............\n",
            "  log_e likelihood: -5.5054e+06\n",
            "  log_2 likelihood: -7.94262e+06\n",
            "     cross entropy: 7.03058\n",
            "        perplexity: 130.742\n",
            "      posterior p0: 0.0300985\n",
            " posterior al-feat: -0.0811994\n",
            "       size counts: 1424\n",
            "  1  model al-feat: -0.121766 (tension=9.84652)\n",
            "  2  model al-feat: -0.113762 (tension=10.6578)\n",
            "  3  model al-feat: -0.108052 (tension=11.3091)\n",
            "  4  model al-feat: -0.103755 (tension=11.8462)\n",
            "  5  model al-feat: -0.1004 (tension=12.2973)\n",
            "  6  model al-feat: -0.0977101 (tension=12.6813)\n",
            "  7  model al-feat: -0.0955103 (tension=13.0115)\n",
            "  8  model al-feat: -0.0936825 (tension=13.2977)\n",
            "     final tension: 13.5474\n",
            "ITERATION 4\n",
            ".................................................. [50000]\n",
            "............\n",
            "  log_e likelihood: -5.10209e+06\n",
            "  log_2 likelihood: -7.36076e+06\n",
            "     cross entropy: 6.51554\n",
            "        perplexity: 91.4899\n",
            "      posterior p0: 0.0334376\n",
            " posterior al-feat: -0.0749326\n",
            "       size counts: 1424\n",
            "  1  model al-feat: -0.0921447 (tension=13.5474)\n",
            "  2  model al-feat: -0.090106 (tension=13.8916)\n",
            "  3  model al-feat: -0.0894828 (tension=14)\n",
            "  4  model al-feat: -0.0894828 (tension=14)\n",
            "  5  model al-feat: -0.0894828 (tension=14)\n",
            "  6  model al-feat: -0.0894828 (tension=14)\n",
            "  7  model al-feat: -0.0894828 (tension=14)\n",
            "  8  model al-feat: -0.0894828 (tension=14)\n",
            "     final tension: 14\n",
            "ITERATION 5 (FINAL)\n",
            ".................................................. [50000]\n",
            "............\n",
            "  log_e likelihood: -5.01541e+06\n",
            "  log_2 likelihood: -7.23571e+06\n",
            "     cross entropy: 6.40485\n",
            "        perplexity: 84.7328\n",
            "      posterior p0: 0\n",
            " posterior al-feat: 0\n",
            "       size counts: 1424\n"
          ]
        }
      ],
      "source": [
        "! ./fast_align -i bible.en-ru -d -o -v > forward.align"
      ],
      "id": "NeJ7kwVO8alp"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDK81vUU8apD",
        "outputId": "f70258d1-b9b7-43bc-ada2-bd49987ce2ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ARG=i\n",
            "ARG=d\n",
            "ARG=o\n",
            "ARG=v\n",
            "ARG=r\n",
            "INITIAL PASS \n",
            ".................................................. [50000]\n",
            "............\n",
            "expected target length = source length * 1.39587\n",
            "ITERATION 1\n",
            ".................................................. [50000]\n",
            "............\n",
            "  log_e likelihood: -3.213e+07\n",
            "  log_2 likelihood: -4.63538e+07\n",
            "     cross entropy: 29.8974\n",
            "        perplexity: 1e+09\n",
            "      posterior p0: 0.08\n",
            " posterior al-feat: -0.167123\n",
            "       size counts: 1424\n",
            "ITERATION 2\n",
            ".................................................. [50000]\n",
            "............\n",
            "  log_e likelihood: -8.57717e+06\n",
            "  log_2 likelihood: -1.23742e+07\n",
            "     cross entropy: 7.98116\n",
            "        perplexity: 252.679\n",
            "      posterior p0: 0.054358\n",
            " posterior al-feat: -0.117062\n",
            "       size counts: 1424\n",
            "  1  model al-feat: -0.121194 (tension=4)\n",
            "  2  model al-feat: -0.119894 (tension=4.08264)\n",
            "  3  model al-feat: -0.119014 (tension=4.13928)\n",
            "  4  model al-feat: -0.118412 (tension=4.17831)\n",
            "  5  model al-feat: -0.117998 (tension=4.2053)\n",
            "  6  model al-feat: -0.117712 (tension=4.22401)\n",
            "  7  model al-feat: -0.117514 (tension=4.237)\n",
            "  8  model al-feat: -0.117376 (tension=4.24603)\n",
            "     final tension: 4.25231\n",
            "ITERATION 3\n",
            ".................................................. [50000]\n",
            "............\n",
            "  log_e likelihood: -6.39184e+06\n",
            "  log_2 likelihood: -9.22148e+06\n",
            "     cross entropy: 5.94769\n",
            "        perplexity: 61.7209\n",
            "      posterior p0: 0.0462711\n",
            " posterior al-feat: -0.107409\n",
            "       size counts: 1424\n",
            "  1  model al-feat: -0.117281 (tension=4.25231)\n",
            "  2  model al-feat: -0.114333 (tension=4.44975)\n",
            "  3  model al-feat: -0.112325 (tension=4.58822)\n",
            "  4  model al-feat: -0.110928 (tension=4.68654)\n",
            "  5  model al-feat: -0.109944 (tension=4.75692)\n",
            "  6  model al-feat: -0.109242 (tension=4.80762)\n",
            "  7  model al-feat: -0.108739 (tension=4.84429)\n",
            "  8  model al-feat: -0.108376 (tension=4.87089)\n",
            "     final tension: 4.89023\n",
            "ITERATION 4\n",
            ".................................................. [50000]\n",
            "............\n",
            "  log_e likelihood: -5.95069e+06\n",
            "  log_2 likelihood: -8.58503e+06\n",
            "     cross entropy: 5.53719\n",
            "        perplexity: 46.4365\n",
            "      posterior p0: 0.0509881\n",
            " posterior al-feat: -0.100136\n",
            "       size counts: 1424\n",
            "  1  model al-feat: -0.108113 (tension=4.89023)\n",
            "  2  model al-feat: -0.10598 (tension=5.04977)\n",
            "  3  model al-feat: -0.104456 (tension=5.16664)\n",
            "  4  model al-feat: -0.103351 (tension=5.25304)\n",
            "  5  model al-feat: -0.10254 (tension=5.31734)\n",
            "  6  model al-feat: -0.10194 (tension=5.36542)\n",
            "  7  model al-feat: -0.101493 (tension=5.4015)\n",
            "  8  model al-feat: -0.101159 (tension=5.42864)\n",
            "     final tension: 5.44911\n",
            "ITERATION 5 (FINAL)\n",
            ".................................................. [50000]\n",
            "............\n",
            "  log_e likelihood: -5.79634e+06\n",
            "  log_2 likelihood: -8.36235e+06\n",
            "     cross entropy: 5.39357\n",
            "        perplexity: 42.0364\n",
            "      posterior p0: 0\n",
            " posterior al-feat: 0\n",
            "       size counts: 1424\n"
          ]
        }
      ],
      "source": [
        "! ./fast_align -i bible.en-ru -d -o -v -r > reverse.align"
      ],
      "id": "hDK81vUU8apD"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "7CpA55Eg8au5"
      },
      "outputs": [],
      "source": [
        "! ./atools -i forward.align -j reverse.align -c grow-diag-final-and > result.align"
      ],
      "id": "7CpA55Eg8au5"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWM8v0dd8axl",
        "outputId": "174cca0f-2744-42cf-e029-3b77c7e7be26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0-0 1-1 2-1 3-3 4-2 5-1 6-4 7-5 8-6 9-6\n",
            "0-1 1-1 2-0 3-2 4-3 5-4 6-3 7-3 8-5 9-8 10-6 13-9 14-9 15-12 16-11 18-13 19-14 24-15\n",
            "0-2 1-0 1-1 2-2 4-3 5-4 5-5 7-7 8-6 9-7\n",
            "0-2 1-0 1-1 3-3 4-7 6-4 7-5 7-6 8-6 9-6 10-9 11-8 12-10 13-10 14-11 15-11 16-12\n",
            "0-2 1-0 1-1 2-3 3-3 3-4 4-3 5-5 6-5 7-6 8-5 10-7 11-8 12-9 13-10 14-11 15-12 16-12 17-13 19-14\n",
            "0-2 1-1 2-0 2-3 3-4 4-4 6-5 7-6 9-6 10-6 11-7 12-7 13-8 14-9 15-11 16-10 18-12 19-13 20-13 21-14\n",
            "0-2 1-0 1-1 3-3 4-4 5-3 5-5 6-4 7-6 8-7 9-8 10-8 11-7 12-8 13-9 14-9 15-8 15-10 16-11 18-12 19-12 20-13 22-16 23-15 24-14 24-16\n",
            "0-2 1-0 1-1 3-3 4-3 4-4 5-5 6-6 7-7 8-8 9-9 10-9 11-10 12-10 13-12 14-11 14-12\n",
            "0-2 1-1 2-0 2-3 4-5 5-7 6-6 7-7 8-3 9-4 10-8 11-9 12-10 13-11 14-12 15-13 17-15 18-15 19-14 19-15 21-18 22-17 23-16 23-18\n",
            "0-2 1-0 1-1 3-3 4-4 5-3 6-5 7-5 8-6 12-7 13-8 14-8 14-10 15-9 16-12 17-11 18-13 19-14 20-15 21-15\n"
          ]
        }
      ],
      "source": [
        "! head result.align"
      ],
      "id": "uWM8v0dd8axl"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HfVyR-3v8a2s"
      },
      "outputs": [],
      "source": [
        "# результат - соответствие позиций слов двух языков для каждого предложения"
      ],
      "id": "HfVyR-3v8a2s"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2hgshUQkYdJ"
      },
      "source": [
        "# Некоторые идеи дальнейшего развития"
      ],
      "id": "Z2hgshUQkYdJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MAnvbNZlqCT"
      },
      "outputs": [],
      "source": [
        "import sqlite3"
      ],
      "id": "2MAnvbNZlqCT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYbzo5w2kkCk"
      },
      "outputs": [],
      "source": [
        "# для пословного перевода - создать базу данных-словарь\n",
        "conn = sqlite3.connect('wordbase.db')\n",
        "cur = conn.cursor()"
      ],
      "id": "jYbzo5w2kkCk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rKfHkvLmhC-"
      },
      "outputs": [],
      "source": [
        "cur.execute(\"\"\"\n",
        "CREATE TABLE words (\n",
        "    word_id INT,\n",
        "    eng_word TEXT,\n",
        "    rus_word TEXT,\n",
        "    eng_pos TEXT,\n",
        "    rus_pos TEXT,\n",
        "    PRIMARY KEY (word_id)) \"\"\")\n",
        "conn.commit()"
      ],
      "id": "_rKfHkvLmhC-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpSIchkNnkhD"
      },
      "outputs": [],
      "source": [
        "# принцип расширения - для нового языка своя таблица с частью речи\n",
        "cur.execute(\"\"\"\n",
        "CREATE TABLE words_fr (\n",
        "    word_id\n",
        "    fr_word TEXT,\n",
        "    pos TEXT,\n",
        "    PRIMARY KEY (word_id)) \"\"\")\n",
        "conn.commit()"
      ],
      "id": "VpSIchkNnkhD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtomTrYMouLB"
      },
      "outputs": [],
      "source": [
        "wds = ('1', 'cat', 'кошка', 'n', 'n',)\n",
        "cur.execute(\"INSERT INTO words VALUES (?, ?, ?, ?, ?)\", wds)\n",
        "conn.commit()"
      ],
      "id": "VtomTrYMouLB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjwzLvGbBLrC",
        "outputId": "fb0a3e25-0c12-46d9-e1da-6142ddcfdf39"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "((0, 1), (0, 1), 'michael', 'michael')\n",
            "((0, 2), (0, 4), 'michael assumes', 'michael geht davon aus')\n",
            "((0, 2), (0, 4), 'michael assumes', 'michael geht davon aus ,')\n",
            "((0, 3), (0, 6), 'michael assumes that', 'michael geht davon aus , dass')\n",
            "((0, 4), (0, 7), 'michael assumes that he', 'michael geht davon aus , dass er')\n",
            "((0, 9), (0, 10), 'michael assumes that he will stay in the house', 'michael geht davon aus , dass er im haus bleibt')\n",
            "((1, 2), (1, 4), 'assumes', 'geht davon aus')\n",
            "((1, 2), (1, 4), 'assumes', 'geht davon aus ,')\n",
            "((1, 3), (1, 6), 'assumes that', 'geht davon aus , dass')\n",
            "((1, 4), (1, 7), 'assumes that he', 'geht davon aus , dass er')\n",
            "((1, 9), (1, 10), 'assumes that he will stay in the house', 'geht davon aus , dass er im haus bleibt')\n",
            "((2, 3), (5, 6), 'that', ', dass')\n",
            "((2, 3), (5, 6), 'that', 'dass')\n",
            "((2, 4), (5, 7), 'that he', ', dass er')\n",
            "((2, 4), (5, 7), 'that he', 'dass er')\n",
            "((2, 9), (5, 10), 'that he will stay in the house', ', dass er im haus bleibt')\n",
            "((2, 9), (5, 10), 'that he will stay in the house', 'dass er im haus bleibt')\n",
            "((3, 4), (6, 7), 'he', 'er')\n",
            "((3, 9), (6, 10), 'he will stay in the house', 'er im haus bleibt')\n",
            "((4, 6), (9, 10), 'will stay', 'bleibt')\n",
            "((4, 9), (7, 10), 'will stay in the house', 'im haus bleibt')\n",
            "((6, 8), (7, 8), 'in the', 'im')\n",
            "((6, 9), (7, 9), 'in the house', 'im haus')\n",
            "((8, 9), (8, 9), 'house', 'haus')\n"
          ]
        }
      ],
      "source": [
        "# вместо синтаксического парсинга использовать инструмент Alignment (например, из nltk) при обучении модели на корпусных данных\n",
        "algnsent = AlignedSent(['klein', 'ist', 'das', 'Haus'], ['the', 'house', 'is', 'small'], Alignment.fromstring('0-3 1-2 2-0 3-1'))\n",
        "srctext = \"michael assumes that he will stay in the house\"\n",
        "trgtext = \"michael geht davon aus , dass er im haus bleibt\"\n",
        "alignment = [(0,0), (1,1), (1,2), (1,3), (2,5), (3,6), (4,9), (5,9), (6,7), (7,7), (8,8)]\n",
        "phrases = phrase_extraction(srctext, trgtext, alignment)\n",
        "for i in sorted(phrases):\n",
        "    print(i)"
      ],
      "id": "UjwzLvGbBLrC"
    }
  ],
  "metadata": {
    "colab": {
      "name": "project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}